# DE4ML-SIGMOD25

Artifacts of Paper "Data Enhancing for Machine Learning"

## 0 Full-Version Paper

`DE4ML-FullVersion.pdf` is the full version of our submitted paper.

## 1 Artifacts Overview

### 1.1 Organization

In the main directory `Artiacts`, you can see the file tree organization as follows

```bash
.
├── data
│   ├── german
│   │   ├── german.csv
│   │   ├── feature_importance.csv
│   │   ├── test
│	│   │	├── german_bim.csv
│	│   │	├── german_carlini.csv
│	│   │	├── german_clean.csv
│	│   │	├── german_deepfool.csv
│	│   │	└── german_fgsm.csv
│   │   └── train
│	│   	├── german_clean.csv
│	│   	├── german_dirty.csv
│	│   	├── german_rahabaran.csv
│	│   	└── german_rock.csv
│   ├── Bank_balanced
│   │   └── ...
│   ├── adult_balanced
│   │   └── ...
│   ├── marketing
│   │   └── ...
│   └── mushroom
│       └── ...
├── de4ml.yml
├── reproduction
│   └── ...
└── src
    ├── adversarial
    │   └── ...
    ├── datamodels
    │   └── ...
    ├── utils
    │   └── ...
    ├── fast_l1
    │   └── ...
    ├── load_checkpoints.sh
    ├── save_checkpoints.sh
    ├── train_datamodels.py
    ├── run.py
    └── de4ml.py
        
```

### 1.2 Auxiliary File

`de4ml.yml` contains the conda environment for quick start.

### 1.3 Data

The `./data` directory contains the data used in our experiments, with each dataset in a separate subdirectory `./data/${DATASET_NAME}`.

> Below, we are located in the `./data/${DATASET_NAME}/` directory.

#### 1.3.1 Original dataset

The file `${DATASET_NAME}.csv` contains the original, unsplit dataset. For an unbalanced dataset, we follow Picket's settings to adjust it to a balanced one. See our paper for more details.

#### 1.3.2 Feature importance file

The file `feature_importance.csv` contains feature importance values generated from training an ML model on the specified dataset. This file is automatically produced using `TabularPredictor.feature_importance()` in AutoGluon, providing an initial assessment of feature significance to help us decide whether a feature is imperceptible or not. Of course, a domain expert can also provide or refine this information to ensure greater accuracy in real-world applications.

#### 1.3.3 `train` and `test` directory

We use `train_test_split` from sklearn with a random seed of 42 to split the original dataset into training and test (prediction) sets, saved respectively in `./train/${DATASET_NAME}_clean.csv` and `./test/${DATASET_NAME}_clean.csv`.

In the `train` directory, `${DATASET_NAME}_dirty.csv` is the dirty file generated by Random Attack (RA). `${DATASET_NAME}_rahabaran.csv` contains the (dirty) file repaired using the `raha` + `baran` algorithm, and `${DATASET_NAME}_rock.csv` is repaired using the `rock` algorithm. These files are mainly used in Phase 1 (DEAAT).

In the `test` directory, the other files follow the naming pattern `${DATASET_NAME}_${ATTACKER_NAME}.csv`. For example, `${DATASET_NAME}_bim.csv` represents the clean test (prediction) data that has been attacked by `bim`. These files are mainly used in Phase 2 (DEAAP).

### 1.4 Codes

The `./src` directory contains the source codes that implement our proposed methods.

#### 1.4.1 `de4ml.py`

This file contains the core implementation of our method. For simplicity, we directly print the results within the `.fit()` method, so we pass in the clean test file (`X_test`, `y_test`) and the training ground truth (`X_train_gt`, `y_train_gt`). We guarantee that there is no data leakage.

#### 1.4.2 `run.py`

This file can help you conduct a quick run of our method. It takes some parameters as input and you can set their values based on the comments and helps in the codes.

#### 1.4.3 `save/load_checkpoints.sh`

These scripts are used to save and load checkpoints to reproduce the experiments. Instructions on how to use them will be provided later.

#### 1.4.4 Other directories / files 

The `adversarial` directory contains various algorithms for attacking the dataset.

The `datamodels` directory, `fast_l1` directory, and `train_datamodels.py` file are used in training the datamodels.

The `utils` directory contains utility functions used in implementing our methods.

## 2 Quick Start

### 2.1 Set the Environment

**First**, installing some necessary dependencies in the terminal (here for Ubuntu):

> These dependencies are required for training the datamodels. If you encounter additional issues after installation, please refer to the [datamodels repository](https://github.com/MadryLab/datamodels) or consult other forums for further assistance.

```bash
sudo apt-get install parallel libopencv-dev 
```

**Then**, make sure you have conda, and run the following command under the `Artifacts` directory in the terminal:

```bash
conda env create -f de4ml.yml -n de4ml
```

After intalling all required dependencies, activate the conda environment:

```bash
conda activate de4ml
```

Now you have an established environment.

*Note: To train datamodels, you may need at least one GPU.*

### 2.2 Run our method from scratch

#### 2.2.1 Phase 1 (DEAAT)

In `./Artifacts/src` directory, run the following examples from scratch:

```bash
python3 run.py -r ../data/ -d german -c rahabaran -mode 'repair' --init_repair True
```

You can change `german` to `marketing`, `mushroom`, `adult_balanced` and `Bank_balanced`.

You can change `rahabaran` to `rock`.

#### 2.2.2 Phase 2 (DEAAP)

In `./Artifacts/src` directory, run the following examples from scratch:

```bash
python3 run.py -r ../data/ -d german -a bim -mode 'enhance' --init_enhance True 
```

You can change `german` to `marketing`, `mushroom`, `adult_balanced` and `Bank_balanced`.

*Note: Here, the ground truth dataset is enhanced, as we do not yet have the results from Phase 1.*

#### 2.2.3 Combine Phase 1 (DEAAT) with Phase 2 (DEAAP)

In `./Artifacts/src` directory, run the following examples from scratch:

```bash
python3 run.py -r ../data/ -d german -c rahabaran -a bim -mode 'all' --init_repair True --init_enhance True
```

You can change `german` to `marketing`, `mushroom`, `adult_balanced` and `Bank_balanced`.

You can change `rahabaran` to `rock`.

*Note: Here, our method first defends against attacks within the training data and then enhances robustness by incorporating adversarial examples to protect against potential attacks in the test (prediction) data.*

### 2.3 Reproduce our method from checkpoints

The checkpoints mainly include datamodels' parameters and trained models. We provide checkpoints to reproduce the experiments reported in our submitted paper. Due to storage constraints, we include only the checkpoints for Fig. 5(a) and Fig. 5(f), which demonstrate the performance of DEAAT and DEAAP across different datasets. These checkpoints are available via an external link in [Dropbox]().

If you need additional checkpoints, please let us know.

#### 2.3.1 Prepare checkpoints

> We are located in the `Artiacts` directory

**First**, enter into the `reproduction` directory

```bash
cd reproduction
```

**Then**, download the checkpoints, put them in the `reproduction` directory, and unzip them.

```bash
# suppose you have fig5a_varydatasets_repair.zip and fig5f_varydatasets_enhance.zip
unzip fig5a_varydatasets_repair.zip
unzip fig5f_varydatasets_enhance.zip
```

**After that**, the file tree organization will look like:

```bash
reproduction
├── fig5a_varydatasets_repair
│   ├── adult_balanced
│   │   └── repair
│   │       ├── datamodels
│   │       └── models
│   ├── Bank_balanced
│   │   └── ...
│   ├── german
│   │   └── ...
│   ├── marketing
│   │   └── ...
│   └── mushroom
│       └── ...
└── fig5f_varydatasets_enhance # we only use 3 datasets for Phase 2
    ├── adult_balanced
    │   └── enhance
    │       ├── datamodels
    │       └── models
    ├── Bank_balanced
    │   └── ...
    ├── german
    │   └── ...
    └── repaired_data
        ├── adult_balanced_repair_15.csv
        ├── Bank_balanced_repair_17.csv
        └── german_repair_21.csv
```

*Note: The `repaired_data` directory within `fig5f_varydatasets_enhance` contains the Phase 1 results of rahabaran+default settings, enabling a direct start to Phase 2.*

#### 2.3.2 Phase 1 (DEAAT)

**First**, load the checkpoints:

```bash
bash load_checkpoints.sh fig5a_varydatasets_repair german repair # you can change the dataset
```

which follows a pattern

```bash
bash load_checkpoints.sh <exp_name> <dataset_name> <mode>
```

`exp_name` specifies the experiment name, representing the subdirectory name under `reproduction`.

`dataset_name` is the name of the dataset. 

`mode` represents the checkpoint phase, with options of `repair` for Phase 1 or `enhance` for Phase 2.

**Then**, run our method:

```bash
python3 run.py -r ../data/ -d german -c rahabaran -mode 'repair'
```

*Note: remember do not use `--init_repair True ` because it will clear all the checkpoints.*

**Finally**, you can get the result:

```bash
{'dirty': 0.615, 'cr_method': 0.625, 'ground_truth': 0.755, 'deaat': 0.71}
```

#### 2.3.3 Phase 2 (DEAAP)

**First**, load the checkpoints:

```bash
bash load_checkpoints.sh fig5f_varydatasets_enhance german enhance # you can change the dataset
```

**Then**, run our method:

```bash
python3 run.py -r ../data/ -d german -c rahabaran -a bim -mode 'all' # enhance from Phase 1 result
```

Besides the dataset, you can also change the attackers from `bim`, `fgsm`, `deepfool` and `carlini`.

**Finally**, you can get the result:

```bash
{'unenhanced': 'rob: 0.52', 'deaap': 'rob: 0.765'}
```

### 2.4 Save your own checkpoints

After running our method from scratch (See Section 2.2), execute the following commands:

```bash
bash save_checkpoints.sh <exp_name> <dataset_name> <mode> # choose mode from 'repair' or 'enhance', no 'all'
```

It is just the reverse process of loading the checkpoints.
